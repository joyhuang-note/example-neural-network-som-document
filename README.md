Example-Neural-Network-SOM Document
=========

程式執行說明
-----------------------

此次作業實做 SOM ，可以對學習次數、學習率和神經元數目進行設定。圖形界面格局如下，最左邊區塊為顯示測驗結果，如各個神經元獲勝的次數，中間區塊為顯示資料坐標和自我組織特徵映射圖，最右邊區塊為使用者自定輸入區塊。操作方式為，填入自定的學習率、學習次數、神經元數目（row、column），按下”訓練”按鈕即產生自我組織特徵映射圖，再按下“測驗”將會計算出各個神經元的獲勝次數。

程式簡介和實驗結果
-----------------------

關於拓璞的方式，是採用矩陣式的拓璞模型，鄰近區域函數的強度，則是使用高斯形式的函數，次外，神經元平面位置向量的距離計算方式則採用歐式距離算法，以上式子進而構成了優勝神經元的鍵結值向量的調整公式，透過學習率的調整和不斷迭代的訓練過程，最後再將以訓練過的各神經元鍵結值向量顯示出來，形成拓璞圖。

以下為程式運行結果，預設學習率為 0.1 、學習次數為 10000 、神經元數目為 100 ( 10 * 10 )

### 2cring 自我組織特徵映射圖 ###

![2cring.png](http://imgur.com/WzgO2OS)

### 2Hcircle1 自我組織特徵映射圖 ###

![2Hcircle1.png](http://imgur.com/5tuQ6mN.png)

### 5CloseS1 自我組織特徵映射圖 ###

![5CloseS1.png](http://imgur.com/Fchnghs.png)

實驗結果分析與討論
-----------------------

此次實作中，部分資料拓璞結果還算清楚，但也有不少的資料拓璞結果會出現網子打結、纏繞的現象，可能在學習率和鄰近函數的設定上還要再加強，應該也跟一剛開始隨機產生的初始鍵結值有很大的相關，而目前自己在運行程式時，會儘量以較小的學習率和較多的學習次數來迭代。此外，將測驗資料帶入已訓練過的神經元時發現，神經元的獲勝次數彼此有很大的落差，譬如有的神經獲勝次數可達數萬次，但有的神經元可能獲勝次數就只有零。
